{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer, TreebankWordTokenizer\n",
    "\n",
    "class SimpleTokenizer:      # Tokenize and remove punctuation  \n",
    "\n",
    "    def __init__(self):\n",
    "        self._tokenizer_ = WordPunctTokenizer()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self._tokenizer_.tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "        return tokens\n",
    "\n",
    "class SimpleTokenizer2:     # Tokenizes and removes punctuation but does not remove contractions\n",
    "\n",
    "    def __init__(self):\n",
    "        self._tokenizer_ = TreebankWordTokenizer()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self._tokenizer_.tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if not (len(token)==1 and not token.isalnum())]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('popular')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\n",
    "\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\n",
    "\"there'd\": \"there would\",\"there'd've\": \"there would have\",\n",
    "\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\n",
    "\"what've\": \"what have\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
    "\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\"you've\": \"you have\"}\n",
    "\n",
    "# Expand contractions helper function\n",
    "def expand_contractions(text, contractions_dict=contractions_dict):\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(re.escape(key) for key in contractions_dict.keys()))\n",
    "    return contractions_re.sub(lambda match: contractions_dict[match.group(0)], text)\n",
    "\n",
    "# Pre-Process Text Parameters are boolean. TRUE if you want to remove that.\n",
    "def preProcessText(text,lowercase=True,contractions=False,punctuations=True,digits=False,stemming=False,Stopwords=True):\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    if contractions:\n",
    "        text = expand_contractions(text)\n",
    "\n",
    "    if Stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = ' '.join(word for word in text.split()if word.lower() not in stop_words)\n",
    "        \n",
    "    if punctuations:\n",
    "        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words = nltk.tokenize.wordpunct_tokenize(text)\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        text = ' '.join(stemmed_words)\n",
    "\n",
    "    if digits:\n",
    "        digits_pattern = r\"\\d+(\\.\\d+)?\"\n",
    "        text = re.sub(digits_pattern, \"<NUMBER>\", text)    \n",
    "\n",
    "    text = ' '.join(word for word in text.split() if (word.isascii() and len(word) > 1))\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def KL_Divergence(document_model, relevance_model_probabilities):\n",
    "    doc_prob = []\n",
    "    rel_prob = []\n",
    "    for token in relevance_model_probabilities.keys():\n",
    "        doc_prob.append(document_model.token_probability(token))\n",
    "        rel_prob.append(relevance_model_probabilities[token])\n",
    "    doc_prob = np.array(doc_prob)\n",
    "    rel_prob = np.array(rel_prob)\n",
    "    \n",
    "    return np.sum(doc_prob * np.log10(doc_prob/rel_prob))\n",
    "\n",
    "def KL_Divergence_Reverse(document_model, query_model):\n",
    "    accumulator = 0\n",
    "    for word in query_model.keys():\n",
    "        if query_model[word] > 0:\n",
    "            accumulator += query_model[word] * math.log10(query_model[word]/document_model.token_probability(word))\n",
    "    return accumulator\n",
    "\n",
    "def parse_results_file(results_file_path):    \n",
    "    results = {}    # query_id -> rank -> doc_id\n",
    "\n",
    "    with open(results_file_path, 'r') as rf:\n",
    "        for line in rf.readlines():\n",
    "            if line.strip():\n",
    "                query_id, ignore_col, doc_id, rank, score, run_id = line.split()\n",
    "                query_id = int(query_id)\n",
    "                rank = int(rank)\n",
    "                if query_id not in results:\n",
    "                    results[query_id] = {}\n",
    "                results[query_id][rank] = doc_id\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class DocLangModel:     # Document Maximnum Likelihood Language Model with Dirichlet Smoothing\n",
    "    def __init__(self,doc_id,doc_text,DIRICHLET_MU=500,tokenizer='S1'):\n",
    "        self.doc_id = doc_id\n",
    "        self.doc_text = preProcessText(doc_text,lowercase=True,punctuations=True,digits=False,stemming=False,Stopwords=True,contractions=True)\n",
    "        self.token_freq = defaultdict(int)\n",
    "        if tokenizer == \"S1\":\n",
    "            self.tokenizer = SimpleTokenizer()\n",
    "        else:\n",
    "            self.tokenizer = SimpleTokenizer2()\n",
    "        tokens = self.tokenizer.tokenize(self.doc_text)\n",
    "        self.doc_length = len(tokens)\n",
    "        for token in tokens:\n",
    "            self.token_freq[token] += 1\n",
    "        self.collection_model = None\n",
    "        self.dirichlet_mu = DIRICHLET_MU\n",
    "\n",
    "    def add_collection_model(self,CollectionLangModel):\n",
    "        self.collection_model = CollectionLangModel\n",
    "    \n",
    "    def token_probability(self,token):\n",
    "        return ((self.token_freq.get(token,0) + self.dirichlet_mu * self.collection_model.calc_M_c(token))/(self.doc_length + self.dirichlet_mu))\n",
    "\n",
    "\n",
    "class CollectionLangModel:     # Maximnum Likelihood Language Model based on term frequencies in the collection as a whole\n",
    "    def __init__(self):\n",
    "        self.coll_total_tokens = 0\n",
    "        self.coll_token_freq = defaultdict(int)\n",
    "\n",
    "    def add_DocLangModel(self,docModel:DocLangModel):\n",
    "        self.coll_total_tokens += docModel.doc_length\n",
    "        for token,freq in docModel.token_freq.items():\n",
    "            self.coll_token_freq[token] += freq\n",
    "\n",
    "    def add_unk(self, percentage):\n",
    "        self.coll_token_freq['<UNK>'] = int(percentage/100 * self.coll_total_tokens)\n",
    "    \n",
    "    def calc_M_c(self,token):\n",
    "        return (self.coll_token_freq.get(token,0)/self.coll_total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict,Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "query_file = \"/Users/jeet/Downloads/COL764-A2-2024/queries.tsv\"\n",
    "top_100_file = \"/Users/jeet/Downloads/COL764-A2-2024/top100docs.tsv\"\n",
    "collection_file = \"/Users/jeet/Downloads/COL764-A2-2024/docs.tsv\"\n",
    "output_file = \"output-file\"\n",
    "expansion_file = \"expansion-file\"\n",
    "\n",
    "Queries = {}    # query_id -> text\n",
    "Top100 = {}     # query_id -> { rank -> (doc_id,score) }\n",
    "DocIDs = set()  # set of doc_id\n",
    "Docs = {}       # doc_id -> { url,title,body }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Queries dictionary\n",
    "with open(query_file,'r') as qf:\n",
    "    tsv_reader = csv.reader(qf,delimiter='\\t')\n",
    "    next(tsv_reader)\n",
    "    for row in tsv_reader:\n",
    "        Queries[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Top100 dictionary & DocIDs set\n",
    "with open(top_100_file,'r') as top:\n",
    "    tsv_reader = csv.reader(top,delimiter='\\t')\n",
    "    next(tsv_reader)\n",
    "    query = ''\n",
    "    rank = 1\n",
    "    for row in tsv_reader:\n",
    "        if row[0]!= query:\n",
    "            query = row[0]\n",
    "            rank = 1\n",
    "            Top100[row[0]] = {}\n",
    "        Top100[row[0]][rank] = (row[1],row[2])\n",
    "        DocIDs.add(row[1])\n",
    "        rank+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Docs dictionary\n",
    "with open(collection_file,'r') as cf:\n",
    "    tsv_reader = csv.reader(cf,delimiter='\\t')\n",
    "    for row in tsv_reader:\n",
    "        if row[0] in DocIDs:\n",
    "            Docs[row[0]] = {'url':row[1],'title':row[2],'body':row[3]}\n",
    "        if len(Docs)==2400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worthy worthy also found in thesaurus medical legal idioms encyclopedia wikipedia th adj worth merit value worthy cause honorable admirable worthy fellow sufficient worth deserving worthy revered worthy acclaim pl eminent distinguished person adv american dictionary english language fifth edition copyright 2016 houghton mifflin harcourt publishing company published houghton mifflin harcourt publishing company rights reserved worthy adj thier thiest1 postpositive often foll infinitive sufficient merit value for something someone specified deserving2 worth value meritn pl thiesoften facetious person distinguished character merit collins english dictionary complete unabridged 12th edition 2014 harper collins publishers 1991 1994 1998 2000 2003 2006 2007 2009 2011 adj adj adequate great merit character value worthy successor commendable excellence merit deserving meritorious effort worthy praise person eminent worth merit position adv worthya combining form worthy used meanings of fit newsworthy trustworthy travel roadworthy seaworthy thing specified theinitial element random house kernerman webster college dictionary 2010 dictionaries ltd copyright 2005 1997 1991 random house inc rights reserved thesaurus antonyms related words synonyms legend switch new thesaurus noun worthy important honorable person word often used humorously he told hisstory conservative worthies local worthies rarely challenged chiefconstable important person influential person personage person whose actions andopinions strongly influence course events adj worthy worth merit value honorable admirable worthy fellow worthy cause estimable deserving respect high regardgood morally admirablehonourable honorable worthy honored entitled honor respect anhonorable man led honorable life honorable service country righteous characterized proceeding accepted standards morality justice the prayer righteous man availeth much james 16valuable great material monetary value especially use exchange valuable diamond unworthy lacking value merit dispel student whose conduct deemedunworthy unworthy forgiveness worthy worthy chosen especially spouse the parents found girlsuitable son desirable suitableeligible qualified allowed worthy chosen eligible run office eligible retirement benefits an eligible bachelor worthy qualities abilities merit recognition way behavior worthy reprobation fact worthy attention fit meeting adequate standards purpose fit subject discussion it fitand proper there water fit drink fit duty do see fit to based word net farlex clipart collection 2003 2012 princeton university farlex inc worthyadjective1 praiseworthy good excellent deserving valuable decent reliable worthwhile respectable upright admirable honourable honest righteous reputable virtuous dependable commendable creditable laudable meritorious estimable worthy members communitypraiseworthy useless dubious demeaning unproductive unworthy disreputable untrustworthy undeserving ignoblenoun1 dignitary notable luminary bigwig informal big shot informal personage big hitter informal heavy hitter informal event brought together worthies manyfields dignitary nobody punter informal pleb non person member rank filebe worthy deserve rate earn justify merit qualify for warrant right to deserving of claim cause worthy support collins thesaurus english language complete unabridged 2nd edition 2002 harper collins publishers 1995 2002worthyadjective1 great value costly inestimable invaluable precious priceless valuable idioms beyond price great price deserving honor respect admiration admirable commendable creditable deserving estimable exemplary honorable laudable meritorious praiseworthy reputable respectable satisfying certain requirements selection eligible fit fitted qualified suitable american roget thesaurus copyright 2013 2014 houghton mifflin harcourt publishing company published houghton mifflin harcourt publishing company rights reserved translations select language digno merecedor noble dignitarioworthy adj worthier compar worthiest superl deserving winner champion merecido successor dignoshe found worthy opponent sabatini en sabatini una oponente de su cause buena causa causa nobleto worthy sth sb ser digno de algo algnworthy attention digno de greatest hits album worthy name un disco de grandes digno de su nombreshe wanted much worthy father ansiaba ser digna hija de su padrethat comment worthy esa fue indigna de ustedthat remark worthy reply ese comentario se merece una respuesta2 good person respetable motive aim encomiable3 iro person honorable venerable hum ilustre personaje collins spanish dictionary complete unabridged 8th edition 2005 william collins sons co ltd 1971 1988 harper collins publishers 1992 1993 1996 1997 2000 2003 2005worth nounvalue books little worth sold fifty dollars worth tickets valoradjective1 equal value to stamps worth cent que vale que tiene un valor de2 good enough for suggestion worth considering exhibition well worth visit digno de merecedor de que merece la adjectiveof value worthless old coins sin valor adjective1 good deserving willingly give money worthy cause noble2 of deserving worthy honour given her merecedor digno de3 of typical of suited to keeping with performance worthy champion digno de4 great enough importance etc thought worthy presented king digno merecedornoun plural highly respected person worthy1 deserving fit for blameworthy act digno de fit appropriate use seaworthy ship apto para adjectivedeserving attention time effort etc worthwhile cause worthwhile ask refuse que vale merece la penafor one worthusing one efforts strength etc swam worth towards shore con toda el alma con todas las fuerzas kernerman english multilingual dictionary 2006 2013 dictionaries ltd want thank tfd existence tell friend us add link page visit webmaster page free fun content link page facebook twitter'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_info = Docs['D750949']\n",
    "text = preProcessText((doc_info['title']+' '+doc_info['body']),lowercase=True,Stopwords=True,contractions=True,punctuations=True,digits=False,stemming=False)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for training\n",
    "corpus = []\n",
    "\n",
    "for query_id, query_text in Queries.items():\n",
    "    corpus.append(preProcessText(query_text,lowercase=True,Stopwords=True,contractions=True,punctuations=True,digits=False,stemming=False).split())\n",
    "\n",
    "for doc_id, doc_info in Docs.items():\n",
    "    corpus.append(preProcessText((doc_info['title']+' '+doc_info['body']),lowercase=True,Stopwords=True,contractions=True,punctuations=True,digits=False,stemming=False).split())\n",
    "                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg:  0\n",
      "window:  3\n",
      "window:  5\n",
      "window:  7\n",
      "window:  9\n",
      "sg:  1\n",
      "window:  3\n",
      "window:  5\n",
      "window:  7\n",
      "window:  9\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec to generate local embeddings\n",
    "for sg in [0,1]:\n",
    "    print('sg: ',sg)\n",
    "    if sg == 1:\n",
    "        mod = \"Skip\"\n",
    "    else:\n",
    "        mod = \"CBOW\"\n",
    "    for window in [3,5,7,9]:\n",
    "        print('window: ',window)\n",
    "\n",
    "        w2v_model = Word2Vec(sentences=corpus, vector_size=300, window=window, min_count=5, sg=sg)\n",
    "\n",
    "        word_to_index = w2v_model.wv.key_to_index\n",
    "        embeddings = []\n",
    "        index_to_word = {}\n",
    "        for word,idx in word_to_index.items():\n",
    "            index_to_word[idx] = word\n",
    "        \n",
    "        for word,idx in word_to_index.items():\n",
    "            embeddings.append(w2v_model.wv[word])\n",
    "\n",
    "        embeddings = np.array(embeddings)    \n",
    "        V, k = embeddings.shape \n",
    "                \n",
    "        query_count = 0\n",
    "        for qid,qtext in Queries.items():   # query_id, query_text\n",
    "            query_count +=1\n",
    "            query = preProcessText(qtext,lowercase=True,punctuations=True,digits=False,stemming=False,Stopwords=True,contractions=True)\n",
    "\n",
    "            original_QT = SimpleTokenizer2().tokenize(query)     # Original Query Terms\n",
    "\n",
    "            query_vector = np.zeros(shape=(V,1))    # binary vector to represent terms present in query\n",
    "            for word, idx in word_to_index.items():\n",
    "                query_vector[idx, 0] = original_QT.count(word.lower())\n",
    "            \n",
    "            temp = np.matmul(embeddings.T,query_vector)     \n",
    "            query_sim_scores = np.matmul(embeddings, temp)      # Similarity scores of trained W2V words to the query terms\n",
    "\n",
    "            flat_sim_matrix = query_sim_scores.flatten()\n",
    "            sorted_idx = np.argsort(flat_sim_matrix)[::-1]      # Sort the similaity scores in descending order and store the indexes\n",
    "\n",
    "            for Top_N in [5,10,15,20]:\n",
    "                sim_word_idx = [(idx, 0) for idx in sorted_idx[:Top_N]]    # Select the top N words\n",
    "                \n",
    "                expanded_QT = []    # Expanded Query Terms\n",
    "                \n",
    "                # Write expansion terms to Expansions file\n",
    "                with open(expansion_file+'_'+mod+'_win_'+str(window)+'@Top_'+str(Top_N),'a') as expansion:\n",
    "                    expansion.write(f\"{qid}: \")\n",
    "                    for term_idx,_ in sim_word_idx:\n",
    "                        expansion.write(f\"{index_to_word[term_idx]} \")\n",
    "                        score = flat_sim_matrix[term_idx]\n",
    "                        expanded_QT.append((index_to_word[term_idx], score))\n",
    "                    expansion.write(\"\\n\")\n",
    "\n",
    "                norm_c = sum([score for word, score in expanded_QT])\n",
    "                EQT_score = {k: v/norm_c for k, v in expanded_QT}   # Normalised scores for expanded query term.    Expansion Term : score\n",
    "                OQT_score = Counter(original_QT)                    # Original Query Term : 1\n",
    "\n",
    "                # Get Language Model for each of the top 100 doc for the query containing token freq and doc length\n",
    "                for DIRICHLET_MU in [250,500,750,1000,1250]:\n",
    "                    doc_lang_models = []\n",
    "                    for rank,(doc_id,score) in Top100[qid].items():\n",
    "                        doc_text = Docs[doc_id]['title'] + \" \" + Docs[doc_id]['body']\n",
    "                        doc_lang_models.append(DocLangModel(doc_id,doc_text,DIRICHLET_MU,'S1'))\n",
    "                    \n",
    "                    # Get Collection Statistics - Collection is of the top 100 docs\n",
    "                    Coll_lang_model  = CollectionLangModel()\n",
    "                    for DLM in doc_lang_models:\n",
    "                        Coll_lang_model.add_DocLangModel(DLM)\n",
    "\n",
    "                    Coll_lang_model.add_unk(0.5)\n",
    "\n",
    "                    # Add Collection Statistics to each Document language Model to calculate M_d(t) = (f_(t,d) + mu * M_c(t))/(l_d + mu)\n",
    "                    for DLM in doc_lang_models:\n",
    "                        DLM.add_collection_model(Coll_lang_model)\n",
    "\n",
    "                    # Recalculate Original Query Term scores only for terms present in the collection else <UNK>\n",
    "                    new_OQT_score = defaultdict(int)\n",
    "                    for QT, score in OQT_score.items():\n",
    "                        key = QT if Coll_lang_model.coll_token_freq.get(QT) is not None else '<UNK>'\n",
    "                        new_OQT_score[key] += score\n",
    "                    OQT_score = new_OQT_score\n",
    "\n",
    "                    # Recalculate Expanded Query Term scores only for terms present in the collection else <UNK>\n",
    "                    new_EQT_score = defaultdict(int)\n",
    "                    for QT, score in EQT_score.items():\n",
    "                        key = QT if Coll_lang_model.coll_token_freq.get(QT) is not None else '<UNK>'\n",
    "                        new_EQT_score[key] += score\n",
    "                    EQT_score = new_EQT_score\n",
    "\n",
    "                    EQT_norm = sum(EQT_score.values())  # EQT scores normalization constant\n",
    "                    OQT_norm = sum(OQT_score.values())  # OQT scores normalization constant\n",
    "\n",
    "                    # compute relevance model probabilities\n",
    "                    for Model_Lambda in [0.15,0.3,0.45,0.6,0.75,0.9]:\n",
    "                        relevance_model_prob = defaultdict(int)\n",
    "                        for token in Coll_lang_model.coll_token_freq.keys():\n",
    "                            relevance_model_prob[token] += (Model_Lambda) * (EQT_score.get(token,0)/EQT_norm)\n",
    "                            relevance_model_prob[token] += (1-Model_Lambda) * (OQT_score.get(token,0)/OQT_norm)\n",
    "\n",
    "                        results = []\n",
    "                        for i in range(len(doc_lang_models)):\n",
    "                            results.append((doc_lang_models[i].doc_id, 1-KL_Divergence_Reverse(doc_lang_models[i], relevance_model_prob)))\n",
    "\n",
    "                        results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                        with open(output_file+'_'+mod+'_win_'+str(window)+'@mu_'+str(DIRICHLET_MU)+'@Top_'+str(Top_N)+'@LAMBDA_'+str(Model_Lambda),'a') as out:\n",
    "                            for idx, (doc_id,score) in enumerate(results):\n",
    "                                out.write(f\"{qid} Q0 {doc_id} {idx+1} {score:.6f} runid1\\n\")\n",
    "                \n",
    "                # print(f\"Processed Query Number: {query_count}\\r\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'e': 1,\n",
       " 'a': 2,\n",
       " 'i': 3,\n",
       " 'r': 4,\n",
       " 's': 5,\n",
       " 't': 6,\n",
       " 'n': 7,\n",
       " 'o': 8,\n",
       " 'l': 9,\n",
       " 'd': 10,\n",
       " 'c': 11,\n",
       " 'm': 12,\n",
       " 'u': 13,\n",
       " 'p': 14,\n",
       " 'g': 15,\n",
       " 'h': 16,\n",
       " 'y': 17,\n",
       " 'b': 18,\n",
       " 'f': 19,\n",
       " 'w': 20,\n",
       " 'v': 21,\n",
       " 'k': 22,\n",
       " '0': 23,\n",
       " '1': 24,\n",
       " '2': 25,\n",
       " 'x': 26,\n",
       " 'j': 27,\n",
       " '9': 28,\n",
       " '5': 29,\n",
       " '3': 30,\n",
       " '4': 31,\n",
       " '7': 32,\n",
       " '8': 33,\n",
       " 'q': 34,\n",
       " '6': 35,\n",
       " 'z': 36}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each Query\n",
    "query_count = 0\n",
    "qid = '42255'\n",
    "qtext = Queries['42255']\n",
    "query_count +=1\n",
    "query = preProcessText(qtext,lowercase=True,punctuations=True,digits=False,stemming=False,Stopwords=True,contractions=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = np.zeros(shape=(V,1)) ### binary vector representing terms present in query\n",
    "for word, idx in word_to_index.items():\n",
    "    query_vector[idx, 0] = query.lower().split().count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_scores = np.matmul(embeddings, embeddings.T)\n",
    "# query_sim_scores = np.matmul(sim_scores, query_vector)\n",
    "\n",
    "temp = np.matmul(embeddings.T,query_vector)\n",
    "query_sim_scores = np.matmul(embeddings, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_sim_matrix = query_sim_scores.flatten()\n",
    "sorted_idx = np.argsort(flat_sim_matrix)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_N = 20\n",
    "sim_word_idx = [(idx, 0) for idx in sorted_idx[:Top_N]]\n",
    "\n",
    "original_QT = query.lower().split()     # Original Query Terms\n",
    "expanded_QT = []    # Expanded Query Terms\n",
    "\n",
    "for term_idx,_ in sim_word_idx:\n",
    "    score = flat_sim_matrix[term_idx]\n",
    "    expanded_QT.append((index_to_word[term_idx], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "norm_c = sum([score for word, score in expanded_QT])\n",
    "EQT_score = {k: v/norm_c for k, v in expanded_QT}\n",
    "OQT_score = Counter(original_QT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lang_models = []\n",
    "for rank,(doc_id,score) in Top100[qid].items():\n",
    "    doc_text = Docs[doc_id]['title'] + \" \" + Docs[doc_id]['body']\n",
    "    doc_lang_models.append(DocLangModel(doc_id,doc_text,'S2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coll_lang_model  = CollectionLangModel()\n",
    "for DLM in doc_lang_models:\n",
    "    Coll_lang_model.add_DocLangModel(DLM)\n",
    "\n",
    "Coll_lang_model.add_unk(0.5)\n",
    "\n",
    "# Add Collection Statistics to each Document language Model to calculate M_d(t) = (f_(t,d) + mu * M_c(t))/(l_d + mu)\n",
    "for DLM in doc_lang_models:\n",
    "    DLM.add_collection_model(Coll_lang_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_OQT_score = defaultdict(int)\n",
    "for QT, score in OQT_score.items():\n",
    "    key = QT if Coll_lang_model.coll_token_freq.get(QT) is not None else '<UNK>'\n",
    "    new_OQT_score[key] += score\n",
    "OQT_score = new_OQT_score\n",
    "\n",
    "new_EQT_score = defaultdict(int)\n",
    "for QT, score in EQT_score.items():\n",
    "    key = QT if Coll_lang_model.coll_token_freq.get(QT) is not None else '<UNK>'\n",
    "    new_EQT_score[key] += score\n",
    "EQT_score = new_EQT_score\n",
    "\n",
    "EQT_norm = sum(EQT_score.values())\n",
    "OQT_norm = sum(OQT_score.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04829463661729081"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EQT_score.get('dental',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_Lambda = 0.5\n",
    "relevance_model_prob = defaultdict(int)\n",
    "for token in Coll_lang_model.coll_token_freq.keys():\n",
    "    relevance_model_prob[token] += (W2V_Lambda) * (EQT_score.get(token,0)/EQT_norm)\n",
    "    relevance_model_prob[token] += (1-W2V_Lambda) * (OQT_score.get(token,0)/OQT_norm)\n",
    "\n",
    "results = []\n",
    "for i in range(len(doc_lang_models)):\n",
    "    results.append((doc_lang_models[i].doc_id, 1-KL_Divergence_Reverse(doc_lang_models[i], relevance_model_prob)))\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'should',\n",
       " 've',\n",
       " 'been',\n",
       " 'there',\n",
       " 'y',\n",
       " 'all',\n",
       " 'know',\n",
       " 'what',\n",
       " 'i',\n",
       " 'm',\n",
       " 'sayin']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = SimpleTokenizer()\n",
    "tok.tokenize(\"I should've been there. Y'all know what I'm sayin'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
