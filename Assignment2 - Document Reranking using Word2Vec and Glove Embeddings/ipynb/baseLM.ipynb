{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "DIRICHLET_MU = 500\n",
    "\n",
    "class DocLangModel:     # Document Maximnum Likelihood Language Model with Dirichlet Smoothing\n",
    "    def __init__(self,doc_id,doc_text):\n",
    "        self.doc_id = doc_id\n",
    "        self.doc_text = preProcessText(doc_text,lowercase=True,punctuations=True,digits=False,stemming=False,Stopwords=True)\n",
    "        self.token_freq = defaultdict(int)\n",
    "        self.tokenizer = SimpleTokenizer()\n",
    "        tokens = self.tokenizer.tokenize(self.doc_text)\n",
    "        self.doc_length = len(tokens)\n",
    "        for token in tokens:\n",
    "            self.token_freq[token] += 1\n",
    "        self.collection_model = None\n",
    "        self.dirichlet_mu = DIRICHLET_MU\n",
    "\n",
    "    def add_collection_model(self,CollectionLangModel):\n",
    "        self.collection_model = CollectionLangModel\n",
    "    \n",
    "    def token_probability(self,token):\n",
    "        return ((self.token_freq.get(token,0) + self.dirichlet_mu * self.collection_model.calc_M_c(token))/(self.doc_length + self.dirichlet_mu))\n",
    "\n",
    "\n",
    "class CollectionLangModel:     # Maximnum Likelihood Language Model based on term frequencies in the collection as a whole\n",
    "    def __init__(self):\n",
    "        self.coll_total_tokens = 0\n",
    "        self.coll_token_freq = defaultdict(int)\n",
    "\n",
    "    def add_DocLangModel(self,docModel:DocLangModel):\n",
    "        self.coll_total_tokens += docModel.doc_length\n",
    "        for token,freq in docModel.token_freq.items():\n",
    "            self.coll_token_freq[token] += freq\n",
    "\n",
    "    def add_unk(self, percentage):\n",
    "        self.coll_token_freq['<UNK>'] = int(percentage/100 * self.coll_total_tokens)\n",
    "    \n",
    "    def calc_M_c(self,token):\n",
    "        return (self.coll_token_freq.get(token,0)/self.coll_total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "# nltk.download('popular')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Pre-Process Text Parameters are boolean. TRUE if you want to remove that.\n",
    "def preProcessText(text,lowercase=True,punctuations=True,digits=False,stemming=False,Stopwords=True):\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    if Stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = ' '.join(word for word in text.split()if word.lower() not in stop_words)\n",
    "        \n",
    "    if punctuations:\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words = nltk.tokenize.wordpunct_tokenize(text)\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        text = ' '.join(stemmed_words)\n",
    "\n",
    "    if digits:\n",
    "        digits_pattern = r\"\\d+(\\.\\d+)?\"\n",
    "        text = re.sub(digits_pattern, \"<NUMBER>\", text)    \n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def KL_Divergence(document_model, relevance_model_probabilities):\n",
    "    doc_prob = []\n",
    "    rel_prob = []\n",
    "    for token in relevance_model_probabilities.keys():\n",
    "        doc_prob.append(document_model.token_probability(token))\n",
    "        rel_prob.append(relevance_model_probabilities[token])\n",
    "        # accumulator += document_model.probability(word) * log10(document_model.probability(word)/relevance_model_probabilities[word])\n",
    "    doc_prob = np.array(doc_prob)\n",
    "    rel_prob = np.array(rel_prob)\n",
    "    \n",
    "    return np.sum(doc_prob * np.log10(doc_prob/rel_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "class SimpleTokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._tokenizer_ = WordPunctTokenizer()\n",
    "\n",
    "    def tokenize(self, text: str)->List[str]:\n",
    "        tokens = self._tokenizer_.tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "query_file = \"/Users/jeet/Downloads/COL764-A2-2024/queries.tsv\"\n",
    "top_100_file = \"/Users/jeet/Downloads/COL764-A2-2024/top100docs.tsv\"\n",
    "collection_file = \"/Users/jeet/Downloads/COL764-A2-2024/docs.tsv\"\n",
    "output_file = \"output-file\"\n",
    "expansion_file = \"expansion-file\"\n",
    "\n",
    "Queries = {}    # query_id -> text\n",
    "Top100 = {}     # query_id -> { rank -> (doc_id,score) }\n",
    "DocIDs = set()  # set of doc_id\n",
    "Docs = {}       # doc_id -> { url,title,body }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'42255': 'average salary for dental hygienist in nebraska',\n",
       " '47210': 'average wedding dress alteration cost',\n",
       " '67316': 'can fever cause miscarriage early pregnancy',\n",
       " '135802': 'definition of laudable',\n",
       " '156498': 'do google docs auto save',\n",
       " '169208': 'does mississippi have an income tax',\n",
       " '174463': 'dog day afternoon meaning',\n",
       " '258062': 'how long does it take to remove wisdom tooth',\n",
       " '324585': 'how much money do motivational speakers make',\n",
       " '330975': 'how much would it cost to install my own wind turbine',\n",
       " '332593': 'how often to button quail lay eggs',\n",
       " '336901': 'how old is vanessa redgrave',\n",
       " '673670': 'what is a alm',\n",
       " '701453': 'what is a statutory deed',\n",
       " '730539': 'what is chronometer who invented it',\n",
       " '768208': 'what is mamey',\n",
       " '877809': 'what metal are hip replacements made of',\n",
       " '911232': 'what type of conflict does della face in o, henry the gift of the magi',\n",
       " '938400': 'when did family feud come out?',\n",
       " '940547': 'when did rock n roll begin?',\n",
       " '997622': 'where is the show shameless filmed',\n",
       " '1030303': 'who is aziz hashim',\n",
       " '1037496': 'who is rep scalise?',\n",
       " '1043135': 'who killed nicholas ii of russia'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Queries dictionary\n",
    "with open(query_file,'r') as qf:\n",
    "    tsv_reader = csv.reader(qf,delimiter='\\t')\n",
    "    next(tsv_reader)\n",
    "    for row in tsv_reader:\n",
    "        Queries[row[0]] = row[1]\n",
    "\n",
    "Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Top100 dictionary & DocIDs set\n",
    "with open(top_100_file,'r') as top:\n",
    "    tsv_reader = csv.reader(top,delimiter='\\t')\n",
    "    next(tsv_reader)\n",
    "    query = ''\n",
    "    rank = 1\n",
    "    for row in tsv_reader:\n",
    "        if row[0]!= query:\n",
    "            query = row[0]\n",
    "            rank = 1\n",
    "            Top100[row[0]] = {}\n",
    "        Top100[row[0]][rank] = (row[1],row[2])\n",
    "        DocIDs.add(row[1])\n",
    "        rank+=1\n",
    "len(DocIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 8.93 s, total: 2min\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "# Make Docs dictionary\n",
    "with open(collection_file,'r') as cf:\n",
    "    tsv_reader = csv.reader(cf,delimiter='\\t')\n",
    "    for row in tsv_reader:\n",
    "        if row[0] in DocIDs:\n",
    "            Docs[row[0]] = {'url':row[1],'title':row[2],'body':row[3]}\n",
    "        if len(Docs)==2400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average', 'salary', 'dental', 'hygienist', 'nebraska']\n",
      "Processed Query : 42255\n",
      "['average', 'wedding', 'dress', 'alteration', 'cost']\n",
      "Processed Query : 47210\n",
      "['fever', 'cause', 'miscarriage', 'early', 'pregnancy']\n",
      "Processed Query : 67316\n",
      "['definition', 'laudable']\n",
      "Processed Query : 135802\n",
      "['google', 'docs', 'auto', 'save']\n",
      "Processed Query : 156498\n",
      "['mississippi', 'income', 'tax']\n",
      "Processed Query : 169208\n",
      "['dog', 'day', 'afternoon', 'meaning']\n",
      "Processed Query : 174463\n",
      "['long', 'take', 'remove', 'wisdom', 'tooth']\n",
      "Processed Query : 258062\n",
      "['much', 'money', 'motivational', 'speakers', 'make']\n",
      "Processed Query : 324585\n",
      "['much', 'would', 'cost', 'install', 'wind', 'turbine']\n",
      "Processed Query : 330975\n",
      "['often', 'button', 'quail', 'lay', 'eggs']\n",
      "Processed Query : 332593\n",
      "['old', 'vanessa', 'redgrave']\n",
      "Processed Query : 336901\n",
      "['alm']\n",
      "Processed Query : 673670\n",
      "['statutory', 'deed']\n",
      "Processed Query : 701453\n",
      "['chronometer', 'invented']\n",
      "Processed Query : 730539\n",
      "['mamey']\n",
      "Processed Query : 768208\n",
      "['metal', 'hip', 'replacements', 'made']\n",
      "Processed Query : 877809\n",
      "['type', 'conflict', 'della', 'face', 'o', 'henry', 'gift', 'magi']\n",
      "Processed Query : 911232\n",
      "['family', 'feud', 'come', 'out']\n",
      "Processed Query : 938400\n",
      "['rock', 'n', 'roll', 'begin']\n",
      "Processed Query : 940547\n",
      "['show', 'shameless', 'filmed']\n",
      "Processed Query : 997622\n",
      "['aziz', 'hashim']\n",
      "Processed Query : 1030303\n",
      "['rep', 'scalise']\n",
      "Processed Query : 1037496\n",
      "['killed', 'nicholas', 'ii', 'russia']\n",
      "Processed Query : 1043135\n",
      "CPU times: user 31.8 s, sys: 571 ms, total: 32.4 s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "# For each Query\n",
    "for qid,qtext in Queries.items():\n",
    "    query = preProcessText(qtext,lowercase=True,punctuations=True,digits=False,stemming=False,Stopwords=True)\n",
    "\n",
    "    # Get Language Model for each of the top 100 doc for the query containing token freq and doc length\n",
    "    doc_lang_models = []\n",
    "    for rank,(doc_id,score) in Top100[qid].items():\n",
    "        doc_text = Docs[doc_id]['title'] + \" \" + Docs[doc_id]['body']\n",
    "        doc_lang_models.append(DocLangModel(doc_id,doc_text))\n",
    "    \n",
    "    # Get Collection Statistics - Collection is of the top 100 docs\n",
    "    Coll_lang_model  = CollectionLangModel()\n",
    "    for DLM in doc_lang_models:\n",
    "        Coll_lang_model.add_DocLangModel(DLM)\n",
    "\n",
    "    Coll_lang_model.add_unk(0.5)\n",
    "\n",
    "    # Add Collection Statistics to each Document language Model to calculate M_d(t) = (f_(t,d) + mu * M_c(t))/(l_d + mu)\n",
    "    for DLM in doc_lang_models:\n",
    "        DLM.add_collection_model(Coll_lang_model)\n",
    "\n",
    "    query_tokens = [token if token in Coll_lang_model.coll_token_freq.keys() else '<UNK>' for token in query.split()]\n",
    "    print(query_tokens)\n",
    "\n",
    "    Query_prob_per_doc = []     # Probability of entering q as a query given that d is a relevant document\n",
    "    for DLM in doc_lang_models:\n",
    "        query_prob = 1\n",
    "        for term in query_tokens:\n",
    "            query_prob *= DLM.token_probability(term)\n",
    "        Query_prob_per_doc.append(query_prob)  \n",
    "    \n",
    "    Avg_query_prob = sum(Query_prob_per_doc)/len(Query_prob_per_doc)\n",
    "\n",
    "    # compute relevant model probabilities\n",
    "    relevance_model_prob = {}\n",
    "\n",
    "    for token in Coll_lang_model.coll_token_freq.keys():\n",
    "        relevance_model_prob[token] = 0\n",
    "        for idx,DLM in enumerate(doc_lang_models):\n",
    "            prob_TGD = DLM.token_probability(token)     # Probability of the token being in the relevant document\n",
    "            relevance_model_prob[token] += prob_TGD * Query_prob_per_doc[idx]\n",
    "        relevance_model_prob[token] /= len(doc_lang_models)\n",
    "        relevance_model_prob[token] /= Avg_query_prob\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(doc_lang_models)):\n",
    "        results.append((doc_lang_models[i].doc_id, 1-KL_Divergence(doc_lang_models[i], relevance_model_prob)))\n",
    "    \n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    with open(output_file,'a') as out:\n",
    "        for idx, (doc_id,score) in enumerate(results):\n",
    "            out.write(f\"{qid} Q0 {doc_id} {idx+1} {score:.6f} runid1\\n\")\n",
    "\n",
    "    print(f\"Processed Query : {qid}\\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'text', 'i', 'want', 'to', 'tokenize', 'my', 'name', 'is', 'chinmay']\n",
      "['he', 'said', 'i', 'm', '50', 'sure', 'that', 'it', 's', '10', '30', 'am', 'early', 'morning', 'on', '03', '05', '2024', 'check', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "class SimpleTokenizer:\n",
    "\n",
    "    def __init__(self, delimiters):\n",
    "        self.delimiters = delimiters\n",
    "        pattern = \"[\" + re.escape(''.join(self.delimiters)) + \"]+\"\n",
    "        self._tokenizer_ = RegexpTokenizer(pattern=pattern, gaps=True)\n",
    "\n",
    "    def tokenize(self, text: str)->List[str]:\n",
    "        tokens = self._tokenizer_.tokenize(text.lower())\n",
    "        return tokens\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    text = \"\"\"This, is the 'text' I want to tokenize; My name is:chinmay. \"\"\"\n",
    "    text2 = \"He said: 'I'm 50% sure that it's 10:30 am - (early morning) on 03/05/2024; check [this]!\"\n",
    "    tokenizer = SimpleTokenizer([\" \", \",\", \".\", \":\", \";\", \"\\\"\", \"\\'\", '/', '-', '%', '(', ')', '[', ']' ])\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens2 = tokenizer.tokenize(text2)\n",
    "    print(tokens)\n",
    "    print(tokens2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'text', 'i', 'want', 'to', 'tokenize', 'my', 'name', 'is', 'chinmay']\n",
      "['he', 'said', 'i', 'm', '50', 'sure', 'that', 'it', 's', '10', '30', 'am', 'early', 'morning', 'on', '03', '05', '2024', 'check', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer2:\n",
    "    def __init__(self):\n",
    "        self.pattern = r\"[\\ ,\\.:;\\\"'/\\-%\\(\\)\\[\\]]+\"\n",
    "        # self.pattern = r\"[ ,.:;\\\"\\']+\"    # Define the delimiter pattern for tokenization     \n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        tokens = re.split(self.pattern, text)\n",
    "        tokens = [token.lower() for token in tokens if token]   # Filter out any empty strings that might have resulted from splitting    \n",
    "        return tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"\"\"This, is the 'text' I want to tokenize; My name is:chinmay. \"\"\"\n",
    "    text2 = \"He said: 'I'm 50% sure that it's 10:30 am - (early morning) on 03/05/2024; check [this]!\"\n",
    "    tokenizer = SimpleTokenizer2()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens2 = tokenizer.tokenize(text2)\n",
    "    print(tokens)\n",
    "    print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ ,\\.:;\"'/\\-%\\(\\)\\[\\]\n"
     ]
    }
   ],
   "source": [
    "print(re.escape(''.join([\" \", \",\", \".\", \":\", \";\", \"\\\"\", \"\\'\", '/', '-', '%', '(', ')', '[', ']' ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
