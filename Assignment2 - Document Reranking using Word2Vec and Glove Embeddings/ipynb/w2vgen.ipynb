{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer, TreebankWordTokenizer\n",
    "\n",
    "class SimpleTokenizer:      # Tokenize and remove punctuation  \n",
    "\n",
    "    def __init__(self):\n",
    "        self._tokenizer_ = WordPunctTokenizer()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self._tokenizer_.tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "        return tokens\n",
    "\n",
    "class SimpleTokenizer2:     # Tokenizes and removes punctuation but does not remove contractions\n",
    "\n",
    "    def __init__(self):\n",
    "        self._tokenizer_ = TreebankWordTokenizer()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self._tokenizer_.tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if not (len(token)==1 and not token.isalnum())]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jeet/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('popular')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\n",
    "\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\n",
    "\"there'd\": \"there would\",\"there'd've\": \"there would have\",\n",
    "\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\n",
    "\"what've\": \"what have\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
    "\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\"you've\": \"you have\"}\n",
    "\n",
    "# Expand contractions helper function\n",
    "def expand_contractions(text, contractions_dict=contractions_dict):\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(re.escape(key) for key in contractions_dict.keys()))\n",
    "    return contractions_re.sub(lambda match: contractions_dict[match.group(0)], text)\n",
    "\n",
    "# Pre-Process Text Parameters are boolean. TRUE if you want to remove that.\n",
    "def preProcessText(text,lowercase=True,contractions=False,punctuations=True,digits=False,stemming=False,Stopwords=True):\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    if contractions:\n",
    "        text = expand_contractions(text)\n",
    "\n",
    "    if Stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = ' '.join(word for word in text.split()if word.lower() not in stop_words)\n",
    "        \n",
    "    if punctuations:\n",
    "        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words = nltk.tokenize.wordpunct_tokenize(text)\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        text = ' '.join(stemmed_words)\n",
    "\n",
    "    if digits:\n",
    "        digits_pattern = r\"\\d+(\\.\\d+)?\"\n",
    "        text = re.sub(digits_pattern, \"<NUMBER>\", text)    \n",
    "\n",
    "    text = ' '.join(word for word in text.split() if (word.isascii() and len(word) > 1))\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def KL_Divergence(document_model, relevance_model_probabilities):\n",
    "    doc_prob = []\n",
    "    rel_prob = []\n",
    "    for token in relevance_model_probabilities.keys():\n",
    "        doc_prob.append(document_model.token_probability(token))\n",
    "        rel_prob.append(relevance_model_probabilities[token])\n",
    "    doc_prob = np.array(doc_prob)\n",
    "    rel_prob = np.array(rel_prob)\n",
    "    \n",
    "    return np.sum(doc_prob * np.log10(doc_prob/rel_prob))\n",
    "\n",
    "def KL_Divergence_Reverse(document_model, query_model):\n",
    "    accumulator = 0\n",
    "    for word in query_model.keys():\n",
    "        if query_model[word] > 0:\n",
    "            accumulator += query_model[word] * math.log10(query_model[word]/document_model.token_probability(word))\n",
    "    return accumulator\n",
    "\n",
    "def parse_results_file(results_file_path):    \n",
    "    results = {}    # query_id -> rank -> doc_id\n",
    "\n",
    "    with open(results_file_path, 'r') as rf:\n",
    "        for line in rf.readlines():\n",
    "            if line.strip():\n",
    "                query_id, ignore_col, doc_id, rank, score, run_id = line.split()\n",
    "                query_id = int(query_id)\n",
    "                rank = int(rank)\n",
    "                if query_id not in results:\n",
    "                    results[query_id] = {}\n",
    "                results[query_id][rank] = doc_id\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "DIRICHLET_MU = 750\n",
    "\n",
    "class DocLangModel:     # Document Maximnum Likelihood Language Model with Dirichlet Smoothing\n",
    "    def __init__(self,doc_id,doc_text,tokenizer='S1'):\n",
    "        self.doc_id = doc_id\n",
    "        self.doc_text = preProcessText(doc_text,lowercase=True,Stopwords=True,contractions=True,punctuations=True,digits=True,stemming=False)\n",
    "        self.token_freq = defaultdict(int)\n",
    "        if tokenizer == \"S1\":\n",
    "            self.tokenizer = SimpleTokenizer()\n",
    "        else:\n",
    "            self.tokenizer = SimpleTokenizer2()\n",
    "        tokens = self.tokenizer.tokenize(self.doc_text)\n",
    "        self.doc_length = len(tokens)\n",
    "        for token in tokens:\n",
    "            self.token_freq[token] += 1\n",
    "        self.collection_model = None\n",
    "        self.dirichlet_mu = DIRICHLET_MU\n",
    "\n",
    "    def add_collection_model(self,CollectionLangModel):\n",
    "        self.collection_model = CollectionLangModel\n",
    "    \n",
    "    def token_probability(self,token):\n",
    "        return ((self.token_freq.get(token,0) + self.dirichlet_mu * self.collection_model.calc_M_c(token))/(self.doc_length + self.dirichlet_mu))\n",
    "\n",
    "\n",
    "class CollectionLangModel:     # Maximnum Likelihood Language Model based on term frequencies in the collection as a whole\n",
    "    def __init__(self):\n",
    "        self.coll_total_tokens = 0\n",
    "        self.coll_token_freq = defaultdict(int)\n",
    "\n",
    "    def add_DocLangModel(self,docModel:DocLangModel):\n",
    "        self.coll_total_tokens += docModel.doc_length\n",
    "        for token,freq in docModel.token_freq.items():\n",
    "            self.coll_token_freq[token] += freq\n",
    "\n",
    "    def add_unk(self, percentage):\n",
    "        self.coll_token_freq['<UNK>'] = int(percentage/100 * self.coll_total_tokens)\n",
    "    \n",
    "    def calc_M_c(self,token):\n",
    "        return (self.coll_token_freq.get(token,0)/self.coll_total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict,Counter\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "query_file = \"/Users/jeet/Downloads/COL764-A2-2024/queries.tsv\"\n",
    "top_100_file = \"/Users/jeet/Downloads/COL764-A2-2024/top100docs.tsv\"\n",
    "collection_file = \"/Users/jeet/Downloads/COL764-A2-2024/docs.tsv\"\n",
    "w2v_embeddings_file = \"/Users/jeet/Downloads/COL764-A2-2024/word2vec.300d.txt\"\n",
    "output_file = \"output-file\"\n",
    "expansion_file = \"expansion-file\"\n",
    "\n",
    "Queries = {}    # query_id -> text\n",
    "Top100 = {}     # query_id -> { rank -> (doc_id,score) }\n",
    "DocIDs = set()  # set of doc_id\n",
    "Docs = {}       # doc_id -> { url,title,body }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries Loaded\n"
     ]
    }
   ],
   "source": [
    "# Make Queries dictionary\n",
    "with open(query_file,'r') as qf:\n",
    "    tsv_reader = csv.reader(qf,delimiter='\\t')\n",
    "    next(tsv_reader)\n",
    "    for row in tsv_reader:\n",
    "        Queries[row[0]] = row[1]\n",
    "\n",
    "print(\"Queries Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Top100 dictionary & DocIDs set\n",
    "with open(top_100_file,'r') as top:\n",
    "    tsv_reader = csv.reader(top,delimiter='\\t')\n",
    "    next(tsv_reader)\n",
    "    query = ''\n",
    "    rank = 1\n",
    "    for row in tsv_reader:\n",
    "        if row[0]!= query:\n",
    "            query = row[0]\n",
    "            rank = 1\n",
    "            Top100[row[0]] = {}\n",
    "        Top100[row[0]][rank] = (row[1],row[2])\n",
    "        DocIDs.add(row[1])\n",
    "        rank+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Docs dictionary\n",
    "with open(collection_file,'r') as cf:\n",
    "    tsv_reader = csv.reader(cf,delimiter='\\t')\n",
    "    for row in tsv_reader:\n",
    "        if row[0] in DocIDs:\n",
    "            Docs[row[0]] = {'url':row[1],'title':row[2],'body':row[3]}\n",
    "        if len(Docs)==2400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Embeddings array and dictionaries to map Embedding vector indexes and the respective words\n",
    "word_to_index = {}\n",
    "embeddings = []\n",
    "index_to_word = {}\n",
    "\n",
    "with open(w2v_embeddings_file, 'r') as w2v:\n",
    "    num_words, num_dimensions = map(int, w2v.readline().split())\n",
    "    idx = 0\n",
    "    for line in w2v:\n",
    "        line = line.strip().split()\n",
    "        word = line[0].lower()\n",
    "        embedding = np.array(list(map(float, line[1:])))\n",
    "        word_to_index[word] = idx\n",
    "        index_to_word[idx] = word\n",
    "        embeddings.append(embedding/np.sqrt(np.sum(embedding**2)))   # normalize embeddings\n",
    "        idx += 1\n",
    "\n",
    "embeddings = np.array(embeddings)    \n",
    "V, k = embeddings.shape     # embeddings is |V| * |k| where k is the dimension of the word embeddings, |V| is vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_count = 0\n",
    "for qid,qtext in Queries.items():   # query_id, query_text\n",
    "    query_count +=1\n",
    "    query = preProcessText(qtext,lowercase=True,Stopwords=True,contractions=True,punctuations=True,digits=True,stemming=False)\n",
    "\n",
    "    original_QT = SimpleTokenizer2().tokenize(query)     # Original Query Terms\n",
    "\n",
    "    query_vector = np.zeros(shape=(V,1))    # binary vector to represent terms present in query\n",
    "    for word, idx in word_to_index.items():\n",
    "        query_vector[idx, 0] = original_QT.count(word.lower())\n",
    "    \n",
    "    temp = np.matmul(embeddings.T,query_vector)     \n",
    "    query_sim_scores = np.matmul(embeddings, temp)      # Similarity scores of trained W2V words to the query terms\n",
    "\n",
    "    flat_sim_matrix = query_sim_scores.flatten()\n",
    "    sorted_idx = np.argsort(flat_sim_matrix)[::-1]      # Sort the similaity scores in descending order and store the indexes\n",
    "    for Top_N in [5,10,15,20]:\n",
    "        sim_word_idx = [(idx, 0) for idx in sorted_idx[:Top_N]]    # Select the top N words\n",
    "        \n",
    "        expanded_QT = []    # Expanded Query Terms\n",
    "        \n",
    "        # Write expansion terms to Expansions file\n",
    "        with open(expansion_file+'@Top_'+str(Top_N),'w') as expansion:\n",
    "            expansion.write(f\"{qid}: \")\n",
    "            for term_idx,_ in sim_word_idx:\n",
    "                expansion.write(f\"{index_to_word[term_idx]} \")\n",
    "                score = flat_sim_matrix[term_idx]\n",
    "                expanded_QT.append((index_to_word[term_idx], score))\n",
    "            expansion.write(\"\\n\")\n",
    "\n",
    "        norm_c = sum([score for word, score in expanded_QT])\n",
    "        EQT_score = {k: v/norm_c for k, v in expanded_QT}   # Normalised scores for expanded query term.    Expansion Term : score\n",
    "        OQT_score = Counter(original_QT)                    # Original Query Term : 1\n",
    "\n",
    "        # Get Language Model for each of the top 100 doc for the query containing token freq and doc length\n",
    "        doc_lang_models = []\n",
    "        for rank,(doc_id,score) in Top100[qid].items():\n",
    "            doc_text = Docs[doc_id]['title'] + \" \" + Docs[doc_id]['body']\n",
    "            doc_lang_models.append(DocLangModel(doc_id,doc_text,'S2'))\n",
    "        \n",
    "        # Get Collection Statistics - Collection is of the top 100 docs\n",
    "        Coll_lang_model  = CollectionLangModel()\n",
    "        for DLM in doc_lang_models:\n",
    "            Coll_lang_model.add_DocLangModel(DLM)\n",
    "\n",
    "        Coll_lang_model.add_unk(0.5)\n",
    "\n",
    "        # Add Collection Statistics to each Document language Model to calculate M_d(t) = (f_(t,d) + mu * M_c(t))/(l_d + mu)\n",
    "        for DLM in doc_lang_models:\n",
    "            DLM.add_collection_model(Coll_lang_model)\n",
    "\n",
    "        # Recalculate Original Query Term scores only for terms present in the collection else <UNK>\n",
    "        new_OQT_score = defaultdict(int)\n",
    "        for QT, score in OQT_score.items():\n",
    "            key = QT if Coll_lang_model.coll_token_freq.get(QT) is not None else '<UNK>'\n",
    "            new_OQT_score[key] += score\n",
    "        OQT_score = new_OQT_score\n",
    "\n",
    "        # Recalculate Expanded Query Term scores only for terms present in the collection else <UNK>\n",
    "        new_EQT_score = defaultdict(int)\n",
    "        for QT, score in EQT_score.items():\n",
    "            key = QT if Coll_lang_model.coll_token_freq.get(QT) is not None else '<UNK>'\n",
    "            new_EQT_score[key] += score\n",
    "        EQT_score = new_EQT_score\n",
    "\n",
    "        EQT_norm = sum(EQT_score.values())  # EQT scores normalization constant\n",
    "        OQT_norm = sum(OQT_score.values())  # OQT scores normalization constant\n",
    "\n",
    "        # compute relevance model probabilities\n",
    "        for Model_Lambda in [0.15,0.3,0.45,0.6,0.75,0.9]:\n",
    "            relevance_model_prob = defaultdict(int)\n",
    "            for token in Coll_lang_model.coll_token_freq.keys():\n",
    "                relevance_model_prob[token] += (Model_Lambda) * (EQT_score.get(token,0)/EQT_norm)\n",
    "                relevance_model_prob[token] += (1-Model_Lambda) * (OQT_score.get(token,0)/OQT_norm)\n",
    "\n",
    "            results = []\n",
    "            for i in range(len(doc_lang_models)):\n",
    "                results.append((doc_lang_models[i].doc_id, 1-KL_Divergence_Reverse(doc_lang_models[i], relevance_model_prob)))\n",
    "\n",
    "            results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            with open(output_file+'@Top_'+str(Top_N)+'@Lambda_'+str(Model_Lambda),'a') as out:\n",
    "                for idx, (doc_id,score) in enumerate(results):\n",
    "                    out.write(f\"{qid} Q0 {doc_id} {idx+1} {score:.6f} runid1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings_file = \"/Users/jeet/Downloads/COL764-A2-2024/glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Embeddings array and dictionaries to map Embedding vector indexes and the respective words\n",
    "word_to_index = {}\n",
    "embeddings = []\n",
    "index_to_word = {}\n",
    "\n",
    "with open(glove_embeddings_file, 'r') as glove:\n",
    "    idx = 0\n",
    "    for line in glove:\n",
    "        line = line.strip().split()\n",
    "        word = line[0].lower()\n",
    "        embedding = np.array(list(map(float, line[1:])))\n",
    "        word_to_index[word] = idx\n",
    "        index_to_word[idx] = word\n",
    "        embeddings.append(embedding/np.sqrt(np.sum(embedding**2)))   # normalize embeddings\n",
    "        idx += 1\n",
    "\n",
    "embeddings = np.array(embeddings)    \n",
    "V, k = embeddings.shape     # embeddings is |V| * |k| where k is the dimension of the word embeddings, |V| is vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_count = 0\n",
    "for qid,qtext in Queries.items():   # query_id, query_text\n",
    "    query_count +=1\n",
    "    query = preProcessText(qtext,lowercase=True,Stopwords=True,contractions=True,punctuations=True,digits=True,stemming=False)\n",
    "\n",
    "    original_QT = SimpleTokenizer2().tokenize(query)     # Original Query Terms\n",
    "\n",
    "    query_vector = np.zeros(shape=(V,1))    # binary vector to represent terms present in query\n",
    "    for word, idx in word_to_index.items():\n",
    "        query_vector[idx, 0] = original_QT.count(word.lower())\n",
    "    \n",
    "    temp = np.matmul(embeddings.T,query_vector)     \n",
    "    query_sim_scores = np.matmul(embeddings, temp)      # Similarity scores of trained W2V words to the query terms\n",
    "\n",
    "    flat_sim_matrix = query_sim_scores.flatten()\n",
    "    sorted_idx = np.argsort(flat_sim_matrix)[::-1]      # Sort the similaity scores in descending order and store the indexes\n",
    "    for Top_N in [5,10,15,20]:\n",
    "        sim_word_idx = [(idx, 0) for idx in sorted_idx[:Top_N]]    # Select the top N words\n",
    "        \n",
    "        expanded_QT = []    # Expanded Query Terms\n",
    "        \n",
    "        # Write expansion terms to Expansions file\n",
    "        with open(expansion_file+'@Top_'+str(Top_N),'w') as expansion:\n",
    "            expansion.write(f\"{qid}: \")\n",
    "            for term_idx,_ in sim_word_idx:\n",
    "                expansion.write(f\"{index_to_word[term_idx]} \")\n",
    "                score = flat_sim_matrix[term_idx]\n",
    "                expanded_QT.append((index_to_word[term_idx], score))\n",
    "            expansion.write(\"\\n\")\n",
    "\n",
    "        norm_c = sum([score for word, score in expanded_QT])\n",
    "        EQT_score = {k: v/norm_c for k, v in expanded_QT}   # Normalised scores for expanded query term.    Expansion Term : score\n",
    "        OQT_score = Counter(original_QT)                    # Original Query Term : 1\n",
    "\n",
    "        # Get Language Model for each of the top 100 doc for the query containing token freq and doc length\n",
    "        doc_lang_models = []\n",
    "        for rank,(doc_id,score) in Top100[qid].items():\n",
    "            doc_text = Docs[doc_id]['title'] + \" \" + Docs[doc_id]['body']\n",
    "            doc_lang_models.append(DocLangModel(doc_id,doc_text,'S2'))\n",
    "        \n",
    "        # Get Collection Statistics - Collection is of the top 100 docs\n",
    "        Coll_lang_model  = CollectionLangModel()\n",
    "        for DLM in doc_lang_models:\n",
    "            Coll_lang_model.add_DocLangModel(DLM)\n",
    "\n",
    "        Coll_lang_model.add_unk(0.5)\n",
    "\n",
    "        # Add Collection Statistics to each Document language Model to calculate M_d(t) = (f_(t,d) + mu * M_c(t))/(l_d + mu)\n",
    "        for DLM in doc_lang_models:\n",
    "            DLM.add_collection_model(Coll_lang_model)\n",
    "\n",
    "        # Recalculate Original Query Term scores only for terms present in the collection else <UNK>\n",
    "        new_OQT_score = defaultdict(int)\n",
    "        for QT, score in OQT_score.items():\n",
    "            key = QT if Coll_lang_model.coll_token_freq.get(QT) is not None else '<UNK>'\n",
    "            new_OQT_score[key] += score\n",
    "        OQT_score = new_OQT_score\n",
    "\n",
    "        # Recalculate Expanded Query Term scores only for terms present in the collection else <UNK>\n",
    "        new_EQT_score = defaultdict(int)\n",
    "        for QT, score in EQT_score.items():\n",
    "            key = QT if Coll_lang_model.coll_token_freq.get(QT) is not None else '<UNK>'\n",
    "            new_EQT_score[key] += score\n",
    "        EQT_score = new_EQT_score\n",
    "\n",
    "        EQT_norm = sum(EQT_score.values())  # EQT scores normalization constant\n",
    "        OQT_norm = sum(OQT_score.values())  # OQT scores normalization constant\n",
    "\n",
    "        # compute relevance model probabilities\n",
    "        for Model_Lambda in [0.15,0.3,0.45,0.6,0.75,0.9]:\n",
    "            relevance_model_prob = defaultdict(int)\n",
    "            for token in Coll_lang_model.coll_token_freq.keys():\n",
    "                relevance_model_prob[token] += (Model_Lambda) * (EQT_score.get(token,0)/EQT_norm)\n",
    "                relevance_model_prob[token] += (1-Model_Lambda) * (OQT_score.get(token,0)/OQT_norm)\n",
    "\n",
    "            results = []\n",
    "            for i in range(len(doc_lang_models)):\n",
    "                results.append((doc_lang_models[i].doc_id, 1-KL_Divergence_Reverse(doc_lang_models[i], relevance_model_prob)))\n",
    "\n",
    "            results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            with open(output_file+'@Top_'+str(Top_N)+'@Lambda_'+str(Model_Lambda),'a') as out:\n",
    "                for idx, (doc_id,score) in enumerate(results):\n",
    "                    out.write(f\"{qid} Q0 {doc_id} {idx+1} {score:.6f} runid1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
